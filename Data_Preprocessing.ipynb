{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Preprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjaySivaraman/DAV/blob/master/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S0i_eWl75ZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbe774c2-c3f0-4c91-eba6-a8ff8ffaea97"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVNJjZqNIw4t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "055855af-9490-42d6-c5d5-ebd81dd29664"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoF-WpDw6J3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import plotly.express as px\n",
        "from google.cloud import bigquery"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOfPPFe4tbMO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "167cb9d5-ace9-45b0-a3b5-066c23b2ffd6"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.0.5)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 17.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (49.1.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n",
            "Building wheels for collected packages: pyLDAvis, funcy\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=8d20ca55ca65ff4f165fa37c8fe291f5abcebf9eccba26bb993aeb344e2615b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=02a972a24f48e77652fe4af7f0ca2476a3f9377c8dff50f8a77fc8c16ab68087\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "Successfully built pyLDAvis funcy\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.14 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr_dxq4_IB_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "1a69d2af-bd76-4738-b6be-46ff06f27a40"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i47kCDcoptEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re, numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim, spacy, logging, warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import lemmatize, simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkM8FOeUC5ID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client(project='primal-hybrid-282705')\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT * FROM `gdelt-bq.extra.sourcesbycountry` \n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "master_data = []\n",
        "for row in query_job:\n",
        "  each_data = []\n",
        "  for each_val in row:\n",
        "    each_data.append(each_val)\n",
        "  master_data.append(each_data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybQ_nxipDKmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sources = pd.DataFrame(master_data)\n",
        "df_sources.columns = ['source','FIPS','CountryName']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va345T-JFpFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "ab763f85-6cd1-4065-bd0a-b89894e3714d"
      },
      "source": [
        "df_sources.head(2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>FIPS</th>\n",
              "      <th>CountryName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>northeastsurfing.com</td>\n",
              "      <td>OC</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>alwihdainfo.com</td>\n",
              "      <td>CD</td>\n",
              "      <td>Chad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 source FIPS CountryName\n",
              "0  northeastsurfing.com   OC        None\n",
              "1       alwihdainfo.com   CD        Chad"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvRAfbAYFwNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_count = df_sources.groupby('CountryName')['source'].nunique().reset_index()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dblog6i5F6QO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "367168ec-915d-4e1f-af05-5bf93b945ab9"
      },
      "source": [
        "source_count[source_count['CountryName'].isin(['India','Indonesia','Vietnam','Fiji','Papua New Guinea','Samoa','Solomon Islands','East Timor'])]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CountryName</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>East Timor</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>Fiji</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>India</td>\n",
              "      <td>686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>Indonesia</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>Papua New Guinea</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>Samoa</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>Solomon Islands</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>Vietnam</td>\n",
              "      <td>219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          CountryName  source\n",
              "57         East Timor       2\n",
              "65               Fiji      15\n",
              "90              India     686\n",
              "91          Indonesia      97\n",
              "155  Papua New Guinea       5\n",
              "172             Samoa      12\n",
              "183   Solomon Islands       6\n",
              "219           Vietnam     219"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMpC87l_HVQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_source_observatory = df_sources[df_sources['CountryName'].isin(['India','Indonesia','Vietnam','Fiji','Papua New Guinea','Samoa','Solomon Islands','East Timor'])]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdzCNWGkJbmV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e1ea008-3851-42f6-ec57-73acdb419f35"
      },
      "source": [
        "df_source_observatory['FIPS'].unique()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['FJ', 'IN', 'WS', 'VM', 'ID', 'TT', 'BP', 'PP'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXLvJzJMJM8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_source_observatory.to_csv('/content/drive/My Drive/World Bank Projects/Covid-19 Analysis/Text Mining/sources_gdelt.csv')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTzcSrk88VWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client(project='primal-hybrid-282705')\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT * FROM `gdelt-bq.covid19.onlinenewsgeo` WHERE CountryCode='IN' AND DATE(DateTime) >= \"2020-01-01\" \n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "\n",
        "master_data = []\n",
        "for row in query_job:\n",
        "    #Row values can be accessed by field name or index.\n",
        "    time = row[0]\n",
        "    link = row[1]\n",
        "    heading = row[2]\n",
        "    language = row[4]\n",
        "    docTone = row[5]\n",
        "    location = row[7]\n",
        "    latitude = row[8]\n",
        "    longitude = row[9]\n",
        "    Adm1Code = row[11]\n",
        "    Adm2Code = row[12]\n",
        "    GeoType = row[13]\n",
        "    ContextualText = row[14]\n",
        "    \n",
        "    master_data.append([time,link,heading,language,docTone,location,latitude,longitude,Adm1Code,Adm2Code,GeoType,ContextualText])\n",
        "\n",
        "df_master_data = pd.DataFrame(master_data)\n",
        "df_master_data.columns = ['time','link','heading','language','docTone','location','latitude','longitude','Adm1Code','Adm2Code','GeoType','ContextualText']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ-gM3xjD3r-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_master_data['link'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NWSPN93IAtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_save_raw = '/content/drive/My Drive/World Bank Projects/Covid-19 Analysis/Text Mining/Data_Raw/PNG.pkl'\n",
        "df_master_data.to_pickle(path_save_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrnl6Y1jO99-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVO81woRO96-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_master_data = pd.read_pickle('/content/drive/My Drive/World Bank Projects/Covid-19 Analysis/Text Mining/Data_Raw/India.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "icjbzAVUi-Gp",
        "colab": {}
      },
      "source": [
        "fips_code = '/content/drive/My Drive/World Bank Projects/Covid-19 Analysis/Text Mining/fips_in.xlsx'\n",
        "df_fips_in = pd.read_excel(fips_code,header=None)\n",
        "df_fips_in.columns = ['Adm1Code','State Name']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWBEELIqjcoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_master_data1 = pd.merge(df_master_data,df_fips_in,on='Adm1Code',how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkLKkWSp-eNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_master_data1['Level'] = df_master_data1['Adm1Code'].apply(lambda x: 'Centre' if x=='IN' else 'States')\n",
        "df_master_data1.groupby('Level')['link'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C_b1TjE_UEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_common = df_master_data1.groupby(['link'])['Level'].nunique().reset_index()\n",
        "len(df_common[df_common['Level']>1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPw1L9NQGbf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_coverage_state = df_master_data1.groupby(['State Name'])['link'].nunique().reset_index().sort_values(by='link')\n",
        "df_coverage_state.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ48b6q65r9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_doc_tone = df_master_data1.groupby(['State Name'])['docTone'].mean().reset_index().sort_values(by='docTone')\n",
        "df_doc_tone.head(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mzrKzcW52Hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cov_tone = pd.merge(df_coverage_state,df_doc_tone,on='State Name')\n",
        "df_cov_tone['Level'] = df_cov_tone['State Name'].apply(lambda x: 'National' if x=='India - National' else 'States')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIvYyHBUmuDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIaR1UZd5-nT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = px.scatter(df_cov_tone, x=\"link\", y=\"docTone\", hover_name=\"State Name\",color='Level')\n",
        "fig.update_layout(\n",
        "    title=\"GDELT Total Coverage - India\",\n",
        "    xaxis_title=\"Number of Articles\",\n",
        "    yaxis_title=\"Average Document Tone\",\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"#7f7f7f\"\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4b8eCmqe09d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_context(x):\n",
        "  if(x!=None):\n",
        "    text_lower = x.lower()\n",
        "    if any(ext in text_lower for ext in ['corrupt','corruption','bribe','bribery','fraud','misappropriation','embezzlement']):\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efcy7_dZdIHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_master_data1['corruption_indicator'] = df_master_data1['heading'].apply(get_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmvq49fqe7Sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_with_corruption = df_master_data1[df_master_data1['corruption_indicator']==True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV-Xk6TAh4_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_with_corruption = df_with_corruption.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wZQM-PchKOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_state_corruption_count = df_with_corruption.groupby(['State Name','Level'])['link'].nunique().reset_index().sort_values(by='link')\n",
        "df_state_corruption_count['State'] = df_state_corruption_count['State Name'].apply(lambda x:x.split(' ')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbSh_dh2hVN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = px.bar(df_state_corruption_count, x=\"State\", y=\"link\", color='Level')\n",
        "fig.update_layout(\n",
        "    title=\"GDELT Fraud and Corruption - India\",\n",
        "    xaxis_title=\"State\",\n",
        "    yaxis_title=\"# of articles on <br> Fraud and Corruption\",\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=15,\n",
        "        color=\"black\"\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgDcHCfmwcdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_with_corruption.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sks8LawJoZX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sent in sentences:\n",
        "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
        "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
        "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
        "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
        "        yield(sent)  \n",
        "\n",
        "# Convert to list\n",
        "data = df_with_corruption.heading.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MecBXUlxqJrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# !python3 -m spacy download en  # run in terminal once\n",
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
        "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "    texts = [bigram_mod[doc] for doc in texts]\n",
        "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "    texts_out = []\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    # remove stopwords once more after lemmatization\n",
        "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
        "    return texts_out\n",
        "\n",
        "data_ready = process_words(data_words)  # processed Text Data!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaYgy7O0qP8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "\n",
        "# Create Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=4, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=10,\n",
        "                                           passes=10,\n",
        "                                           alpha='symmetric',\n",
        "                                           iterations=100,\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2oI38AyqW_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "df_dominant_topic.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6fn6O9Rq_5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display setting to show more characters in column\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
        "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd:\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index    \n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4nKlR4mq_zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Wordcloud of Top N words in each topic\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "cloud = WordCloud(stopwords=stop_words,\n",
        "                  background_color='white',\n",
        "                  width=2500,\n",
        "                  height=1800,\n",
        "                  max_words=10,\n",
        "                  colormap='tab10',\n",
        "                  color_func=lambda *args, **kwargs: cols[i],\n",
        "                  prefer_horizontal=1.0)\n",
        "\n",
        "topics = lda_model.show_topics(formatted=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    fig.add_subplot(ax)\n",
        "    topic_words = dict(topics[i][1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "    plt.gca().imshow(cloud)\n",
        "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "    plt.gca().axis('off')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CnJXwKxr9rN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sentence Coloring of N Sentences\n",
        "def topics_per_document(model, corpus, start=0, end=1):\n",
        "    corpus_sel = corpus[start:end]\n",
        "    dominant_topics = []\n",
        "    topic_percentages = []\n",
        "    for i, corp in enumerate(corpus_sel):\n",
        "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
        "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
        "        dominant_topics.append((i, dominant_topic))\n",
        "        topic_percentages.append(topic_percs)\n",
        "    return(dominant_topics, topic_percentages)\n",
        "\n",
        "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
        "\n",
        "# Distribution of Dominant Topics in Each Document\n",
        "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
        "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
        "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
        "\n",
        "# Total Topic Distribution by actual weight\n",
        "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
        "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
        "\n",
        "# Top 3 Keywords for each Topic\n",
        "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
        "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
        "\n",
        "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
        "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
        "df_top3words.reset_index(level=0,inplace=True)\n",
        "\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n",
        "\n",
        "# Topic Distribution by Dominant Topics\n",
        "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
        "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
        "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
        "ax1.xaxis.set_major_formatter(tick_formatter)\n",
        "ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
        "ax1.set_ylabel('Number of Documents')\n",
        "ax1.set_ylim(0, 1000)\n",
        "\n",
        "# Topic Distribution by Topic Weights\n",
        "ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
        "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
        "ax2.xaxis.set_major_formatter(tick_formatter)\n",
        "ax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2pO9Rbasea8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get topic weights and dominant topics ------------\n",
        "from sklearn.manifold import TSNE\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models import Label\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "# Get topic weights\n",
        "topic_weights = []\n",
        "for i, row_list in enumerate(lda_model[corpus]):\n",
        "    topic_weights.append([w for i, w in row_list[0]])\n",
        "\n",
        "# Array of topic weights    \n",
        "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
        "\n",
        "# Keep the well separated points (optional)\n",
        "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
        "\n",
        "# Dominant topic number in each doc\n",
        "topic_num = np.argmax(arr, axis=1)\n",
        "\n",
        "# tSNE Dimension Reduction\n",
        "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
        "tsne_lda = tsne_model.fit_transform(arr)\n",
        "\n",
        "# Plot the Topic Clusters using Bokeh\n",
        "output_notebook()\n",
        "n_topics = 4\n",
        "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
        "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
        "              plot_width=900, plot_height=700)\n",
        "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
        "show(plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQH75RALs9vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WLCBRvpuKCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_with_corruption['dominant_topic'] = df_dominant_topic['Dominant_Topic'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0BKn1EsuUiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_with_corruption.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvRI3s1rv8vu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_with_corruption.groupby(['State Name','dominant_topic'])['heading'].nunique().reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YD97zEOx41K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}